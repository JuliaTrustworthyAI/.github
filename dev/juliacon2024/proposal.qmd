---
format: 
  commonmark:
    variant: -raw_html+tex_math_dollars
    wrap: none
bibliography: bib.bib
---

## Abstract

[Taija](https://github.com/JuliaTrustworthyAI) is a growing ecosystem of packages geared towards *T*rustworthy *A*rtificial *I*ntelligence in *J*uli*a*. This talk will provide an update on recent developments of some of our core packages. In particular, the focus will be on contributions made by two groups of students from TU Delft, as well as ongoing JSoC work.

## Description

Julia's ecosystem for Artificial Intelligence (AI) and Machine Learning (ML) has been growing at a relatively rapid pace, perhaps most notably so in the [SciML](https://sciml.ai/) domain but also more broadly (e.g. [Flux](https://fluxml.ai/), [MLJ](https://juliaai.github.io/MLJ.jl/stable/), [JuliaGenAI](https://github.com/JuliaGenAI/juliagenai.org), ...). The goal of Taija is to make AI models more trustworthy, where we have so far focused primarily on the interplay with packages in the latter category.

## üè† About 

Taija currently covers a range of approaches towards making AI systems more trustworthy:

- Model Explainability ([CounterfactualExplanations.jl](https://github.com/JuliaTrustworthyAI/CounterfactualExplanations.jl))
- Algorithmic Recourse ([CounterfactualExplanations.jl](https://github.com/JuliaTrustworthyAI/CounterfactualExplanations.jl), [AlgorithmicRecourseDynamics.jl](https://github.com/JuliaTrustworthyAI/AlgorithmicRecourseDynamics.jl))
- Predictive Uncertainty Quantification ([ConformalPrediction.jl](https://github.com/JuliaTrustworthyAI/ConformalPrediction.jl), [LaplaceRedux.jl](https://github.com/JuliaTrustworthyAI/LaplaceRedux.jl))
- Effortless Bayesian Deep Learning ([LaplaceRedux.jl](https://github.com/JuliaTrustworthyAI/LaplaceRedux.jl))
- Hybrid Learning ([JointEnergyModels.jl](https://github.com/JuliaTrustworthyAI/JointEnergyModels.jl))

Various meta packages can be used to extend the core functionality:

- Plotting ([TaijaPlotting.jl](https://github.com/JuliaTrustworthyAI/TaijaPlotting.jl))
- Datasets for testing and benchmarking ([TaijaData.jl](https://github.com/JuliaTrustworthyAI/TaijaData.jl))
- Parallelization ([TaijaParallel.jl](https://github.com/JuliaTrustworthyAI/TaijaParallel.jl))
- Interoperability with other programming languages ([TaijaInteroperability.jl](https://github.com/JuliaTrustworthyAI/TaijaInteroperability.jl))

The [TaijaBase.jl](https://github.com/JuliaTrustworthyAI/TaijaBase.jl) package provides common symbols, types and functions that are used across all or multiple Taija packages.

## üõ†Ô∏è Recent Developments

A lot has happened since we presented some of our core packages at JuliaCon in 2022 and 2023. During the past year, the focus has been on extending and improving these existing packages. Various groups of undergraduate students at TU Delft as well as a few external collaborates have contributed to these efforts. 

### CounterfactualExplanations.jl

`CounterfactualExplanations.jl` is a package for Counterfactual Explanations and Algorithmic Recourse in Julia. 

- The code base has been streamlined to make it more accessible to new contributors. (students)
- Part of this involved moving functionality out of the package into new meta-packages. (students)
- Multiple new counterfactual generators including FeatureTweak for decision trees and random forests. (students)
- Improved interoperability with Python and R through `TaijaInteroperability.jl`. (students)
- Integration of ECCCo generator for faithful model explanations [@altmeyer2024faithful]. 
- Huge improvements to computational speed and native support for multi-threading and multi-processing.

### LaplaceRedux.jl

`LaplaceRedux.jl` is a package for effortless Bayesian Deep Learning through Laplace Approximation for `Flux.jl` neural networks. 

- Support for multi-class problems. (students)
- Support for more sophisticated and scalable Hessian approximations. (students)
- Interface to MLJ for easy model training and evaluation. (students)

### Students' Experience

I will also briefly report back on the students' experience working on these projects. This includes the challenges they faced, the skills they acquired, and the impact they had on the Taija ecosystem.

## üéØ Ongoing and Future Projects

Taija has been running two Julia Season of Code projects this summer.

1. (*Conformal Bayes*) Bridging the gap between Bayesian and frequentist approaches to Predictive Uncertainty Quantification.
2. (*Causal Recourse*) From minimal perturbations to minimal interventions for Algorithmic Recourse.

Finally, we are also working with a group of students on [TaijaInteractive.jl](https://github.com/JuliaTrustworthyAI/TaijaInteractive.jl): a Genie-based web application for interactive model explanations.

## üìö References

<!-- Programming in Julia has definitely helped us become better programmers: the languages we usually used during our bachelor's courses were either ones that strongly encourage imperative programming (Java, Python) or ones that strongly encourage functional programming (Haskell, Scala). Julia, in my opinion, tries to take a middle road between those languages, and the programming patterns that emerge from this design choice were cool to learn about.
Building an early-stage package in a relatively new language was also an interesting experience: there was a lot more trial and error compared to our usual programming projects, and it was sometimes hard to find the information that would help us find the right approach. However, whenever we had such questions and asked them from the wider Julia community, there were always people ready to help in my experience, which was nice. -->